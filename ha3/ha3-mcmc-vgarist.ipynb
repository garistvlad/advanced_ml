{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADE. Advanced ML. Home assignment #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Adventure of the Dancing Men\n",
    "\n",
    "<img src=\"./data/img/dancing_men.png\" width=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Реализуйте базовый частотный метод по Шерлоку Холмсy\n",
    "\n",
    "- подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "- возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "- расшифруйте их таким частотным методом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.ion()\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_DATAPATH = \"data/corpora\"\n",
    "WAR_AND_PEACE_RU_FILEPATH = os.path.join(CORPUS_DATAPATH, \"WarAndPeace.txt\")\n",
    "WAR_AND_PEACE_EN_FILEPATH = os.path.join(CORPUS_DATAPATH, \"WarAndPeaceEng.txt\")\n",
    "ANNA_KARENINA_RU_FILEPATH = os.path.join(CORPUS_DATAPATH, \"AnnaKarenina.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(filepath: str) -> str:\n",
    "    with open(filepath, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, languare: str = 'ru') -> str:\n",
    "    \"\"\"Process text with following steps:\n",
    "    1. To lower case\n",
    "    2. Take only letters from appropriate languare\n",
    "    3. Replace sequence of subspaces to one\n",
    "    \"\"\"\n",
    "    processed = text.lower()\n",
    "    if languare == \"ru\":\n",
    "        pattern_to_delete = re.compile(\"[^а-яё ]\")\n",
    "    elif languare == \"en\":\n",
    "        pattern_to_delete = re.compile(\"[^a-z ]\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown languale: {languare}\")\n",
    "    processed = re.sub(pattern_to_delete, \" \", processed)\n",
    "    processed = re.sub(r\"\\s+\", \" \", processed)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8d4d569a89408c96c63d846ec84aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CORPUS = dict()\n",
    "\n",
    "for text_id, text_filepath, lang in tqdm(zip(\n",
    "    [\"war_and_peace_ru\", \"war_and_peace_en\", \"anna_karenina_ru\"],\n",
    "    [WAR_AND_PEACE_RU_FILEPATH, WAR_AND_PEACE_EN_FILEPATH, ANNA_KARENINA_RU_FILEPATH],\n",
    "    [\"ru\", \"en\", \"ru\"]\n",
    ")):\n",
    "    text_i = read_text(text_filepath)\n",
    "    CORPUS[text_id] = preprocess_text(text_i, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "war_and_peace_ru\n",
      " война и мир самый известный роман льва николаевича толстого как никакое другое произведение писател \n",
      "\n",
      "war_and_peace_en\n",
      " the project gutenberg ebook of war and peace by leo tolstoy this ebook is for the use of anyone any \n",
      "\n",
      "anna_karenina_ru\n",
      " анна каренина один из самых знаменитых романов льва толстого начинается ставшей афоризмом фразой вс \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_id, text_processed in CORPUS.items():\n",
    "    print(text_id)\n",
    "    print(text_processed[:100], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Частоты по корпусам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_letter_frequencies(text: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate relative frequencies of characters from text\"\"\"\n",
    "    letter_counter = Counter(text)\n",
    "    letter_counter = {i: j / len(text) for i, j in letter_counter.items()}\n",
    "    letter_counter = dict(\n",
    "        sorted(\n",
    "            letter_counter.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "    )\n",
    "    return letter_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Зашифруем и попробуем расшифровать тестовые данные случайной перестановкой символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet(language: str) -> List[str]:\n",
    "    \"\"\"Generate list of letters for appripriate languare\"\"\"\n",
    "    if language == \"ru\":\n",
    "        alphabet = [chr(i) for i in range(ord('а'), ord('я') + 1)] + ['ё', \" \"]\n",
    "    elif language == \"en\":\n",
    "        alphabet = [chr(i) for i in range(ord('a'), ord('z') + 1)] + [\" \"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown languale: {languare}\")\n",
    "    return alphabet\n",
    "\n",
    "\n",
    "def get_shuffled_encoder(language: str = \"ru\") -> Dict[str, str]:\n",
    "    \"\"\"Provide mapping from original alphabet to shuffled one\"\"\"\n",
    "    mapping = dict()\n",
    "    alphabet = get_alphabet(language)\n",
    "    new_alphabet = alphabet.copy()\n",
    "    random.shuffle(new_alphabet)\n",
    "    return dict(zip(alphabet, new_alphabet))\n",
    "\n",
    "\n",
    "def encode_text(text: str, encoder: Dict[str, str]) -> str:\n",
    "    \"\"\"Encode text based on encoder latters mapping\"\"\"\n",
    "    result = \"\"\n",
    "    for i in text:\n",
    "        result += encoder.get(i, \"?\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def decode_text(\n",
    "    encoded_text: str,\n",
    "    train_freq: Dict[str, str]\n",
    ") -> str:\n",
    "    \"\"\"Decode text using letter frequencies previously calculated\"\"\"\n",
    "    test_freq = calculate_letter_frequencies(encoded_text)\n",
    "    \n",
    "    decoder = dict()\n",
    "    test_letters = list(test_freq.keys())\n",
    "    train_letters = list(train_freq.keys())\n",
    "    for i, j in enumerate(test_letters):\n",
    "        if i < len(train_letters):\n",
    "            decoder[j] = train_letters[i]\n",
    "        else:\n",
    "            decoder[j] = \"?\"\n",
    "    \n",
    "    result = encode_text(encoded_text, decoder)\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_accuracy(text_true, text_pred):\n",
    "    \"\"\"Accuracy score of exact letters matching\"\"\"\n",
    "    assert len(text_true) == len(text_pred), \"Text lengths should match exactly\"\n",
    "    return sum([i == j for i, j in zip(text_true, text_pred)]) / len(text_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_encoder = get_shuffled_encoder(\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_and_peace_ru_freq = calculate_letter_frequencies(CORPUS['war_and_peace_ru'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Example:  война и мир самый известный роман льва николаевича\n",
      "Encoded: гьджсщгчгъчлгвщъяжгчфьтв сяжглдъщсгюпьщгсчцдющтьчущ\n",
      "Decoded:  война и мир самый известный роман льва николаевича\n",
      "Example accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Пример из Train dataset-а\n",
    "example_from_train = CORPUS['war_and_peace_ru'][:51]\n",
    "example_from_train_encoded = encode_text(example_from_train, random_encoder)\n",
    "example_from_train_decoded = decode_text(\n",
    "    CORPUS['war_and_peace_ru'],\n",
    "    train_freq=war_and_peace_ru_freq\n",
    ")[:51]\n",
    "\n",
    "print(\"Train Example:\", example_from_train)\n",
    "print(\"Encoded:\", example_from_train_encoded)\n",
    "print(\"Decoded:\", example_from_train_decoded)\n",
    "print(f\"Example accuracy: {calculate_accuracy(example_from_train, example_from_train_decoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Example:  анна каренина один из самых знаменитых романов льва толстого начинается ставшей\n",
      "Encoded: гщссщгцщлтсчсщгдычсгчфгвщъянгфсщътсч янглдъщсдьгюпьщг дюв додгсщучсщт вкгв щьзтж\n",
      "Decoded:  еиие кераиние одни нч семьх чиемаинтьх ромеиов лгве толстоыо иебниеатся стевшаж\n",
      "Example accuracy: 0.525\n"
     ]
    }
   ],
   "source": [
    "# Пример из Test dataset-а, где частоты посчитаны на целом тексте:\n",
    "example_from_test = CORPUS['anna_karenina_ru'][:80]\n",
    "example_from_test_encoded = encode_text(example_from_test, random_encoder)\n",
    "example_from_test_decoded = decode_text(\n",
    "    CORPUS['anna_karenina_ru'],\n",
    "    train_freq=war_and_peace_ru_freq\n",
    ")[:80]\n",
    "\n",
    "print(\"Test Example:\", example_from_test)\n",
    "print(\"Encoded:\", example_from_test_encoded)\n",
    "print(\"Decoded:\", example_from_test_decoded)\n",
    "print(f\"Example accuracy: {calculate_accuracy(example_from_test, example_from_test_decoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "\n",
    "- подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "- проведите тестирование аналогично п.1, но при помощи биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_frequencies(text: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate relative frequencies of bigrams from text\"\"\"\n",
    "    bigram_counter = Counter()\n",
    "    for i in range(len(text) - 1):\n",
    "        bigram_counter[text[i: i + 2]] += i\n",
    "    \n",
    "    n_bigrams = sum(bigram_counter.values())\n",
    "    bigram_counter = dict(\n",
    "        sorted(\n",
    "            bigram_counter.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "    )\n",
    "    bigram_counter = {i: j / n_bigrams for i, j in bigram_counter.items()}\n",
    "    return bigram_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc -> ab bc\n",
    "# ab bc -> abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anna_karenina_encoded = encode_text(CORPUS['anna_karenina_ru'], random_encoder)\n",
    "# anna_karenina_encoded_bigrams_freq = calculate_bigram_frequencies(anna_karenina_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# war_and_peace_ru_bigram_freq = calculate_bigram_frequencies(CORPUS['war_and_peace_ru'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(anna_karenina_encoded) - 1):\n",
    "#     bigram_i = anna_karenina_encoded[i: i + 2]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_decoder = dict(zip(list(anna_karenina_encoded_bigrams_freq), list(war_and_peace_ru_bigram_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тг гв --> и   п\n"
     ]
    }
   ],
   "source": [
    "# i = 100\n",
    "# print(anna_karenina_encoded[i: i + 2], anna_karenina_encoded[i + 1: i + 3], '-->', end=' ')\n",
    "# print(\n",
    "#     bigram_decoder.get(anna_karenina_encoded[i: i + 2], '??'),\n",
    "#     bigram_decoder.get(anna_karenina_encoded[i + 1: i + 3], '??')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet_decoder_from_bigrams = dict()\n",
    "# for i, j in bigram_decoder.items():\n",
    "#     for a, b in zip(i, j):\n",
    "#         if a in alphabet_decoder_from_bigrams:\n",
    "#             alphabet_decoder_from_bigrams[a].append(b)\n",
    "#         else:\n",
    "#             alphabet_decoder_from_bigrams[a] = [b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet_decoder_from_bigrams_result = dict()\n",
    "# not_assigned = list()\n",
    "# for i, j in alphabet_decoder_from_bigrams.items():\n",
    "#     j_updated = [k for k in j if k not in alphabet_decoder_from_bigrams_result.values()]\n",
    "#     if len(j_updated) == 0:\n",
    "#         not_assigned.append(i)\n",
    "#         continue\n",
    "#     alphabet_decoder_from_bigrams_result[i] = Counter(j_updated).most_common(1)[0][0]\n",
    "    \n",
    "# alphabet_decoder_from_bigrams_result.update(\n",
    "#     dict(zip(\n",
    "#         not_assigned,\n",
    "#         [i for i in get_alphabet(\"ru\") if i not in alphabet_decoder_from_bigrams_result.values()]\n",
    "#     ))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' тсст ктнесаст охас аз итгыш зстгесауыш ногтсов юмвт уоюиуоро стчастеуид иутвяеэ тщоназгог щнтзоэ ви'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode_text(anna_karenina_encoded, alphabet_decoder_from_bigrams_result)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "- предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "- реализуйте и протестируйте его, убедитесь, что результаты улучшились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Расшифруйте сообщение:\n",
    "\n",
    "←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸ ↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊ ↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒ ↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝← ⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏\n",
    "\n",
    "Или это (они одинаковые, второй вариант просто на случай проблем с юникодом):\n",
    "\n",
    "დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭ􏰀ႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭ􏰀ႣჵისႼჰႨჲდႩჳჲႨ􏰀ႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨ ႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵ􏰀ႧჂჲდႨ􏰀ႣႩჳჂ􏰀ႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხს დდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩ􏰀ႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Бонус: а что если от биграмм перейти к триграммам (тройкам букв) или даже больше? Улучшатся ли результаты? Когда улучшатся, а когда нет? Чтобы ответить на этот вопрос эмпирически, уже может понадобиться погенерировать много тестовых перестановок и последить за метриками, глазами может быть и не видно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "Бонус: какие вы можете придумать применения для этой модели? Пляшущие человечки ведь не так часто встречаются в жизни (хотя встречаются! и это самое потрясающее во всей этой истории, но об этом я расскажу потом)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
